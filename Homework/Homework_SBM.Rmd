---
title: "Homework: Inference in the Stochastic Block Model"
author: "MAP573 -- Introduction to unsupervised learning"
date: "École Polytechnique - Autumn 2019"
fontsize: 11pt
lang: en
geometry: left=1.2in,top=1.35in,right=1.2in,bottom=1.35in
classoption: a4paper
linkcolor: red
urlcolor: blue
citecolor: green
output:
  pdf_document:
    number_sections: true
    citation_package: natbib
    includes:
      in_header: ../labs/preamble.sty

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preliminaries {-}

\textsf{Goals.}

1. Random graphs generation: Erdös-Rényi, Stochastic Block Models
2. Variational inference for the stochastic block model
3. Analysis of some real world network

\textsf{Instructions.} Each student _must_ send an `R markdown` report generated via `R studio` to <julien.chiquet@inra.fr> at the end of the tutorial. This report should answer the questions by commentaries and codes generating appropriate graphical outputs. [A cheat sheet of the  markdown syntax can be found here.](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)

\textsf{Required packages.} Check that the following packages are available on your computer:
```{r, message=FALSE}
library(igraph)
library(sand)
library(Matrix)
library(blockmodels)
library(aricode)
```

You also need `Rstudio`, \LaTeX\ and packages for markdown: 
```{r, message=FALSE}
library(knitr)
library(rmarkdown)
```
If (and only if!!) you have time, you can also play with 
```{r, message=FALSE}
library(ggraph)
```

# Background

## Notations

We let  $\mathcal{P} =  \{1,\dots,p\}$ be  a set  of nodes. Presence  or absence  of an  edge  between two  nodes $i$  and $j$  is described by the random variable $X_{ij} = \mathbf{1}_{\{i \leftrightarrow j\}}$. We  assume  by  convention  that   the  nodes  are  not  connected  to themselves,  that is,  $X_{ii}=0$ for  all $i\in\mathcal{P}$.

## Stochastic Block Model

This model  has several  representation. We  adopt the  one given by Daudin, Picard and Robin (2007), known as ``mixture model for random  graphs''. This  model spreads  the nodes  among a  set of  $Q$ classes $\mathcal{Q}=\{1,\dots,Q\}$ with  \emph{a priori} distribution ${\boldsymbol \alpha} = (\alpha_1,\dots,\alpha_Q)$.  The hidden random indicator   variables  $(Z_{iq})_{i\in\mathcal{P},   q\in\mathcal{Q}}$ define the classes each node belongs to. Thus
\begin{equation}
  \label{eq:prior_classes}
  \alpha_q  = \mathbb{P}(Z_{iq}  =  1) =  \mathbb{P}(i  \in q),  \quad
  \text{ such that} \sum_{q} \alpha_q = 1.
\end{equation}

It is straightforward to see that $\mathbf{Z}_i =  (Z_{i1}, \dots, Z_{iQ})$ has a multinomial distribution
\begin{equation}
  \label{eq:Zmulti}
  \mathbf{Z}_i \sim \mathcal{M}(1,\boldsymbol\alpha).
\end{equation}

Finally, let  $\pi_{q\ell}$ be the  probability that a node  in class $q$ connects to a node  in class $\ell$\footnote{Since the network is undirected,  the  matrix  $\mathbf{X}$  is  symmetric  and  so $\pi_{q\ell}  = \pi_{\ell  q}$.}. The  probability for  having edge between  nodes $i$  and $j$  is defined  \emph{conditionally on}  the classes they belong to:
\begin{equation}
  \label{eq:mixnet}
  X_{ij} |  \{i\in q, j\in\ell\}  \sim \mathcal{B}(\pi_{q\ell}), \quad i\neq j.
\end{equation}

To sum up, the parameters are

  - $\mathbf{X}=(X_{ij})$,  the  $p\times  p$  adjacency matrix of the graph,
  - ${\boldsymbol\pi} = (\pi_{q\ell})$ the $Q\times Q$ connectivity matrix,
  - ${\boldsymbol \alpha} = (\alpha_q)$, the size-$Q$ vector of class proportions.

### Useful quantities in the variational EM

During this practical, you will implement the variational EM (VEM) algorithm studied during this morning lecture. Here are the expressions of the key quantities that you need to compute along the algorithm.

#### Variational lower bound.

The variational lower bound of the loglikelihood maximized by the VEM is
\begin{equation*}
  J(\boldsymbol\tau,\boldsymbol\pi,\boldsymbol\alpha) = \sum_{i,q} \tau_{iq} \log \alpha_q + \sum_{i<j,q,\ell}  \tau_{iq}  \tau_{j\ell} \log b(X_{ij} ; \pi_{q\ell}) - \sum_{i,q} \tau_{iq} \log(\tau_{iq}),
\end{equation*}
where $b(x;\pi) = \pi^x (1-\pi)^{1-x}$ is the probability density function of the Bernoulli distribution and $\tau_{iq}$ are the posterior probabilities for class belonging, aka the variational parameters.

#### M step.

For  fixed  values of  $\hat\tau_{iq}$ the  estimators    for   $\alpha_q$   and
$\pi_{q\ell}$ by  maximizing the  conditional expectation are
\begin{equation}
\hat{\alpha}_q = \frac{1}{n}\sum_i \hat{\tau}_{iq} , \quad \hat\pi_{q\ell} = \frac{\sum_{i\neq j} \hat{\tau}_{iq}\hat{\tau}_{j\ell} X_{ij}}{\sum_{i\neq j} \hat{\tau}_{iq}\hat{\tau}_{j\ell}}.
\end{equation}

#### E step.

The variational parameters $\tau_{iq}$ verify the following fixed point relation:
\begin{equation}
\hat{\tau}_{iq} \varpropto \alpha_q \prod_{j} \prod_{\ell} b(X_{ij}, \pi_{q\ell})^{\hat{\tau}_{j\ell}}
\end{equation}

#### Integrated complete likelihood criterion (ICL).

The variational ICL used to compare models with different numbers of clusters is
\begin{multline*}
  \mathrm{vICL}(Q) = \sum_{i,q} \hat{\tau}_{iq} \log \hat{\alpha}_q + \sum_{i<j,q,\ell}  \hat{\tau}_{iq}  \hat{\tau}_{j\ell} b(X_{ij} ; \hat{\pi}_{q\ell})
  \\ - \frac{1}{2} \left(\frac{Q(Q+1)}{2} \log \frac{n(n-1)}{2} + (Q-1) \log (n) \right).
\end{multline*}

# Simulations

## Random graph generation.

Write a function which takes the parameters $p, \alpha, \pi$ for arguments and draw a random graph encoded as an igraph object. The vertices must own an attribute for their class membership. You may rely on the function `igraph::sample_sbm` which does almost all the job. Here is a skeleton for your own function:
```{r rNetwork, eval = FALSE}
rNetwork <- function(n, pi, alpha = c(1)) {
### fill this up
}
```

## Examples
Draw some graphs with the following topologies, by choosing :

- Erdös-Rényi random networks,
- Affiliation (or community) networks, i.e. with clusters of highly connected nodes,
- Star networks, i.e. networks where a few nodes are highly connected to the others, the later being unconnected with each other.

These networks can be represented via an image of the corresponding adjacency matrix (try the `image` method of the **Matrix** package), or thanks to the `R`-package **igraph**.

# Variational Inference in the Stochastic block model

Let us now write our own algorithm to fit an SBM on a `igraph` object. To do so, I give you the main algorithm here after. You just need to fill the functions `E_step`, `M_step`, `get_J` and compute the variational ICL and BIC.
```{r VEM skeleton, eval = FALSE}
VEM_SBM <- function(G, Q, cl.init, nstart=5, eps=1e-5, maxIter=50){
  ## Initialization
  stopifnot(is.igraph(G))
  n <- gorder(G)
  J <- vector("numeric", maxIter)
  zero <- .Machine$double.eps
  ### variational lower bound
  get_J <- function(theta, tau){
    ## fill up below
    ## ....
    ## fill up above
    J
  }
  ### M step: update theta (pi and alpha)
  M_step <- function(tau){
    ## fill up below
    ## ....
    ## fill up above
    alpha[alpha < zero] <- zero
    pi[pi < zero] <- zero
    pi[pi > 1- zero] <- 1- zero
    list(pi = pi, alpha = alpha)
  }
  ### E step: update the clustering parameters (tau)
  E_step <- function(theta, tau){
    ## fill this up
    tau
  }
  VEM <- function(tau) {
    cond <- FALSE; iter <- 0
    while(!cond){
      iter <- iter +1
      theta <- M_step(tau)
      # E step
      tau <- E_step(theta, tau) # M step
      J[iter] <- get_J(theta, tau) # assess convergence
      if (iter > 1)
        cond <- (iter > maxIter) | (abs((J[iter] - J[iter-1])) < eps)
    }
    return(list(theta = theta, tau = tau, J = J[1:iter]))
  }
  if (missing(cl.init)) {
    out <- replicate(nstart, {
      cl0 <- sample(1:Q, n, replace = TRUE)
      tau <- matrix(0,n,Q); tau[cbind(1:n, cl0)] <- 1
      VEM(tau)
    })
    best <- out[,which.max(sapply(out['J',], max))]
  } else {
    tau <- matrix(0,n,Q); tau[cbind(1:n, cl.init)] <- 1
    best <- VEM(tau)
  }
  ## FILL THIS UPS FOR THE BEST MODEL
  # vBIC <-
  # vICL <-
  return(list(theta = best$theta,
              tau = best$tau, membership = apply(best$tau, 1, which.max),
              J = best$J, vICL = vICL, vBIC= vBIC))
}
```

# Numerical experiments

## Simple tests

Use the `rNetwork` function to generate some SBM with various structures. Show that you can recover the original parameter thanks to the **blockmodels** package. Then, check that
your own VEM function is also working. You can initialize the clusters with the
spectral clustering function developed during the first tutorial.

## Numerical study: real worl network analysis

Consider a network of your own or one found in the **sand** package, for instance the French political blogosphere. Symmetrize the network and remove the isolated nodes. Then, apply the variational EM. Chose the number of classes with the vICL. Comment.

## Simulations 1: assessing parameter estimation.

Consider for instance an affiliation network with 3 classes. Do some simulations showing that the mean square error of $\hat{\pi}_{q\ell}$ decrease when $n$ increases. To do so, assume that you know the correct number of classes in your graph.

## Simulations 2: assessing clustering efficiency.

Again, consider an affiliation network with 3 classes. Do some simulations showing that the adjusted rand index between the classification recovered by the VEM, when the number of classes is chosen so as to maximize the ICL, increases when $n$ increases. You may use the function `ARI` from the **aricode** package to compare two classifications.

